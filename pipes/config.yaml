# ==================== [ Values that must checked ] =========================

# A list of GenBank accessions representing the reference
# genome, where each accession represents one segment/chromosome.
accessions_for_ref_genome_build:
  - "KJ660346.2"

# An email address used when the pipeline fetches reference data from the NCBI
# The NCBI requires this to ensure rate limiting occurs fairly on a per-user basis
email_point_of_contact_for_ncbi: "someone@example.com"

# Directory path containing the bmTagger databases used for depletion
# of human reads and metagenomic contaminants
# See: ftp://ftp.ncbi.nih.gov/pub/agarwala/bmtagger/README.bmtagger.txt
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/GRCh37.68_ncRNA-GRCh37.68_transcripts-HS_rRNA_mitRNA.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/hg19.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/metagenomics_contaminants_v3.tar.gz
# Can be a remote path prefix (s3://, gs://, etc.). 
# The path prefix should contain files with each of the needed bmtagger extensions added:
#   ['bitmask','srprism.ssa','srprism.imp','srprism.idx','srprism.map','srprism.ss','srprism.amp','srprism.rmp','srprism.pmp','srprism.ssd']
bmtagger_dbs_remove:
  - "s3://sabeti-public-dbs/bmtagger/hg19"
  - "s3://sabeti-public-dbs/bmtagger/GRCh37.68_ncRNA-GRCh37.68_transcripts-HS_rRNA_mitRNA"
  - "s3://sabeti-public-dbs/bmtagger/metagenomics_contaminants_v3"

bwa_dbs_remove:
  - "s3://sabeti-public-dbs/bwa/hg19"

# Path for the directory containing blast databases used for additional depletion
# See: ftp://ftp.ncbi.nih.gov/blast/documents/formatdb.html
#      http://www.compbio.ox.ac.uk/analysis_tools/BLAST/formatdb.shtml
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/metag_v3.ncRNA.mRNA.mitRNA.consensus.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/hybsel_probe_adapters.tar.gz
# Can be a remote path prefix (s3://, gs://, etc.). 
# The path prefix should contain files with each of the needed blast extensions added:
#   ['nhr','nsq','nin']
blast_db_remove: 
  - "s3://sabeti-public-dbs/blast/metag_v3.ncRNA.mRNA.mitRNA.consensus"
  - "s3://sabeti-public-dbs/blast/hybsel_probe_adapters"

# A fasta file containing sequences of adapters/primers/barcodes to be removed via Trimmomatic
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/contaminants.fasta.tar.gz
# Can be a remote path to a fasta file (s3://, gs://, etc.). 
trim_clip_db: "s3://sabeti-public-dbs/trim_clip/contaminants.fasta"

# a fasta file containing sequences to report downstream as spike-ins
# Can be a remote path to a fasta file (s3://, gs://, etc.). 
spikeins_db: "s3://sabeti-public-dbs/spikeins/ercc_spike-ins.fasta"

# Directory of kraken database for metagenomics identification
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/kraken_viral_diversity.tar.gz
# Can be a remote path dirname (s3://, gs://, etc.). 
kraken_db: "s3://sabeti-public-dbs/kraken"

# Path of DIAMOND database file for metagenomics identification
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/nr.dmnd.gz
# Can be a remote path prefix (s3://, gs://, etc.). 
diamond_db: "s3://sabeti-public-dbs/diamond/nr-20170529"

# Directory of the krona database for generating html reports.
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/krona_taxonomy_20160502.tar.gz
# Can be a remote path dirname (s3://, gs://, etc.). 
krona_db: "s3://sabeti-public-dbs/krona"

# BWA index prefix for metagenomic alignment.
# This contains the full human reference genome, multiple diverse sequences covering human
# pathogenic viruses, as well as LSU and SSU rRNA sequences from SILVA.
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/human_viral_rrna.tar.lz4
# Can be a remote path prefix (s3://, gs://, etc.). 
align_rna_db: "s3://sabeti-public-dbs/rna_bwa/human_viral_rrna"

# Directory of the NCBI taxonomy dmp files
# For pre-packaged hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/taxonomy.tar.lz4
# Can be a remote path dirname (s3://, gs://, etc.). 
taxonomy_db: "s3://sabeti-public-dbs/taxonomy"

# These are variables that must be set
env_vars:
  # The directory path to the location of the GATK jar file.
  # It must be explicitly given since GATK has to be licensed and downloaded
  # manually out of band.
  GATK_PATH: "/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.6-0-g89b7209"

  # The directory path to the Novocraft utilities, including Novoalign.
  # Without this path specified, single-threaded unlicensed Novoalign will be used.
  # To use multi-threaded Novoalign, it must be licensed and downloaded
  # manually out of band.
  NOVOALIGN_PATH: "/idi/sabeti-scratch/shared-resources/software/novocraft_v3"

# ==================== [ Values that may be changed ] ==========================

#    |----------------- Project-specific pipeline files ------------------------

# Text file containing sequencer flowcell metadata
# TSV file with the following structure:
#flowcell        lane    barcode_file    bustard_dir     max_mismatches
#H32G3ADXY       1       /path/to/barcodes.txt    /path/to/illumina/run/directory/run_BH32G3ADXY 1
seqruns_demux: "flowcells.txt"

# The location of a text file containing names of samples
# to be run through the depletion stages of the pipeline
samples_depletion: "samples-depletion.txt"

# The location of a text file containing the names of samples
# to be run through the assembly stages of the pipeline.
# Only samples included in this file will be run through
# assembly, and any failing samples are blocking for
# downstream steps. Removing sample names and re-running
# the pipeline will allow processing to continue past
# failures.
samples_assembly: "samples-assembly.txt"

# The location of a text file containing the names of samples
# that have failed to assemble. Not currently used for much
# beyond tracking which samples failed. Sample names removed
# from samples_assembly should be added to this file
samples_assembly_failures: "samples-assembly-failures.txt"

# The location of a text file containing names of samples to run through the
# metagenomics pipeline.
samples_metagenomics: "samples-metagenomics.txt"

# The location of a text file containing the names of sample
# names to be included in the production of VCF files
# during the ref_guided_diversity step.
samples_per_run: "samples-runs.txt"

#    |------------------- Knobs to turn to change tool execution ---------------

# The lastal tool filters input reads to be more representative of the genus of
# interest. By default, the accesstions given for the reference genome are used
# (accessions_for_ref_genome_build). Optionally, a path to a text file can be
# given, the text file itself listing accessions to be used in place of the
# reference accessions for building the lastal database. If the species of
# interest has significant genetic diversity, the lastal filtering step can be
# made more inclusive by expanding the list of accessions given in this file to
# a set that better represents possible diversity. If assemblies are failing due
# to insufficient input data, consider expanding the list of accessions given to lastal.
accessions_file_for_lastal_db_build: ""

# File containing the list of accessions for reference genomes to use for reference selection.
# Defaults to the same list as `accessions_for_ref_genome_build`.
# For multi-segment genomes, the file must list all segments for the first reference, then all segments
# for the second reference, and so on; for each reference, segments must be listed in the same order as for
# `accessions_for_ref_genome_build`; all segments must be listed for each reference.
accessions_file_for_refsel_db_build: ""

# De novo assembler to use.  Current options are "trinity", "spades" and "trinity-spades"
# (the "trinity-spades" option gives contigs produced by Trinity as auxiliary input to SPAdes).
assembly_assembler: "trinity"

# The minimum length an assembled sequence must have
# to be considered acceptible quality, specified as
# a fraction of the length of the reference sequene.
# This is specific to a particular segment/chromosome.
assembly_min_length_fraction_of_reference: 0.50

# The minimum fraction of unambiguous (non-N) bases an assembled
# sequence must have to be considered acceptable quality.
assembly_min_unambig: 0.50

# The replace_length is the number of bases on each end of each segment
# of the reference genome where we throw away de-novo contigs and use only
# reference-based read alignments during assembly.
#assembly_replace_length: 55

# parameters passed to Novoalign during the assembly refinement stages
refine_assembly_1_novoalign_params: "-r Random -l 30 -g 40 -x 20 -t 502"
refine_assembly_2_novoalign_params: "-r Random -l 40 -g 40 -x 20 -t 100"

# parameters passed to Novoalign for mapping reads back to the consensus
map_reads_to_self_novoalign_params: "-r Random -l 40 -g 40 -x 20 -t 100 -k"

# parameters passed to Novoalign for refining the reference-guided consensus
ref_guided_consensus_novoalign_params: "-r Random -l 30 -g 40 -x 20 -t 502"

# Alignment parameters for nucmer (MUMmer) during the scaffolding step.
# Shown below are defaults.
#assembly_nucmer_max_gap:     200
#assembly_nucmer_min_match:   10
#assembly_nucmer_min_cluster: 65
#scaffold_min_pct_contig_aligned: 0.3

# The number of threads to use for tools supporting multiple threads.
# Should equal the number of CPU cores present on the machine(s)
# running the pipeline.
number_of_threads: 8

# MAFFT offset value, which works like gap extension penalty,
# for group-to-group alignment
# See: http://mafft.cbrc.jp/alignment/software/manual/manual.html
mafft_ep: 0.123

# MAFFT number cycles of iterative refinement are performed
# See: http://mafft.cbrc.jp/alignment/software/manual/manual.html
mafft_maxiters: 1000

# The number of reads to be subsampled from input bam files
# and used as input to assembly via Trinity.
# Roughly, ~1/2 hour to 1 hour required per million reads.
# See: https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-FAQ#ques_why_so_many_transcripts
trinity_n_reads: 250000

# The number of reads to be subsampled from input bam files
# and used as input to assembly via SPAdes
spades_n_reads: 10000000

# Minimum number of reads on each strand
vphaser_min_reads_each: 5

# Maximum allowable ratio of number of reads on the two
# strands. Ignored if vphaser_max_bins=0.
vphaser_max_bins: 10

# A simple filter for the VCF merge step.
# If set to true, keep only the alleles that have at least two
# independent libraries of support and 
# allele freq > 0.005. If false, no filtering is performed.
vcf_merge_naive_filter: false

# Strategy for kraken classification for multiple inputs. 'single' results in 1 kraken process for
# each input bam, while 'multiple' coalesces multiple input classifications into a single kraken
# run.
kraken_execution: single

# Random seed, for non-deterministic steps.  0 means use current time.
random_seed: 0

#    |----------------------- Data storage locations ---------------------------

# The parent directory containing data sub-directories.
# Can be relative to the Snakefile location, or absolute.
data_dir: "data"

# Sub-directories within data_dir to store the input and output
# files for various stages of the pipeline.
# For raw sequencer output, the files should go in demux or source,
# depending on whether or not they have been demultiplexed
subdirs:
  demux:          "00_demux"
  source:         "00_raw"
  depletion:      "01_cleaned"
  per_sample:     "01_per_sample"
  assembly:       "02_assembly"
  align_self:     "02_align_to_self"
  metagenomics:   "02_metagenomics"
  align_ref:      "03_align_to_ref"
  multialign_ref: "03_multialign_to_ref"
  interhost:      "03_interhost"
  intrahost:      "04_intrahost"
  annot:          "05_genbank"

# The location to use for temp file storage. Should be large and fast and
# shared between jobs.
# At the Broad, consider /broad/hptmp/myusername
tmp_dir: "tmp"

# The directory to store log files for specific jobs
log_dir: "log"

# The directory to store end-of-pipeline reports.
reports_dir: "reports"

# The directory containing the viral-ngs files (intrahost.py, taxon_filter.py, etc.)
bin_dir: "bin"

# The directory of the virtual environment to be
# activated before execution of the pipeline
conda_env_dir: "conda-env"

# The directory of a (mini)conda install
miniconda_dir: "mc3"

# The directory of where the reference genome fasta files should be stored.
# Can be a remote path prefix (s3://, gs://, etc.). The path should contain a reference.fasta file
ref_genome_dir: "ref_genome"

# The directory where the lastal database files should be stored
# Can be a remote path prefix (s3://, gs://, etc.). The path should contain lastal.{ext} files
lastal_ref_db_dir: "lastal_db"

# The directory where the reference selection files should be stored
# Can be a remote path prefix (s3://, gs://, etc.). The path should contain a refsel.fasta file.
refsel_ref_db_dir: "refsel_db"


#    |------------ Values needed for automated GenBank submission --------------

# When writing output files such as sam/bam files
# The name of the sequencing center that produced the reads.
seq_center: "BI"

# Fields related to preparing files for automated GenBank submission.
genbank:
  author_template: "NCBI/authors.sbt"
  source_modifier_table: "NCBI/sample_meta.src"
  biosample_map: "NCBI/biosample-map.txt"
  sequencing_technology: "Illumina HiSeq 2500; Nextera LC"
  comment: "Please be aware that the annotation is done automatically with little or no manual curation."

# ================== [ Institution-specific parameters ] =======================

#    |----------------- Cluster execution parameters----------------------------

## The project name passed to the cluster scheduler (currently unused)
#project: "viral_ngs"

## Broad-specific LSF cluster scheduler parameters
#LSF_queues:
#  short: "-W 4:00"
#  long: "-q forest"
#  bigmem: "-q flower"

## Broad-specific UGER cluster scheduler parameters
#UGER_queues:
#  short: "-l h_rt=04:00:00"
#  long: "-l h_rt=36:00:00"
