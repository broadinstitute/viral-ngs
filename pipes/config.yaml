# ==================== [ Values that must checked ] =========================

# A list of GenBank accessions representing the reference
# genome, where each accession represents one segment/chromosome.
accessions_for_ref_genome_build:
  - "KJ660346.2"

# An email address used when the pipeline fetches reference data from the NCBI
# The NCBI requires this to ensure rate limiting occurs fairly on a per-user basis
email_point_of_contact_for_ncbi: "someone@example.com"

# Directory path containing the bmTagger databases used for depletion
# of human reads and metagenomic contaminants
# See: ftp://ftp.ncbi.nih.gov/pub/agarwala/bmtagger/README.bmtagger.txt
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/GRCh37.68_ncRNA-GRCh37.68_transcripts-HS_rRNA_mitRNA.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/hg19.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/metagenomics_contaminants_v3.tar.gz
bmtagger_dbs_remove:
  - "/idi/sabeti-scratch/shared-resources/depletion-databases/hg19"
  - "/idi/sabeti-scratch/shared-resources/depletion-databases/GRCh37.68_ncRNA-GRCh37.68_transcripts-HS_rRNA_mitRNA"
  - "/idi/sabeti-scratch/shared-resources/depletion-databases/metagenomics_contaminants_v3"

# Path for the directory containing blast databases used for additional depletion
# See: ftp://ftp.ncbi.nih.gov/blast/documents/formatdb.html
#      http://www.compbio.ox.ac.uk/analysis_tools/BLAST/formatdb.shtml
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/metag_v3.ncRNA.mRNA.mitRNA.consensus.tar.gz
#  - https://storage.googleapis.com/sabeti-public/depletion_dbs/hybsel_probe_adapters.tar.gz
blast_db_remove: 
  - "/idi/sabeti-scratch/shared-resources/depletion-databases/metag_v3.ncRNA.mRNA.mitRNA.consensus"
  - "/idi/sabeti-scratch/shared-resources/depletion-databases/hybsel_probe_adapters"

# A fasta file containing sequences of adapters/primers/barcodes to be removed via Trimmomatic
# For pre-built hosted databases, you may download:
#  - https://console.cloud.google.com/m/cloudstorage/b/sabeti-public/o/depletion_dbs/contaminants.fasta.tar.gz
trim_clip_db: "/idi/sabeti-scratch/shared-resources/depletion-databases/contaminants.fasta"

# a fasta file containing sequences to report downstream as spike-ins
spikeins_db: "/idi/sabeti-scratch/genomes/other/ercc_spike-ins.fasta"

# Directory of kraken database for metagenomics identification
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/kraken_viral_diversity.tar.gz
kraken_db: "/idi/sabeti-scratch/yesimon/db/kraken_viral_diversity"

# Path of DIAMOND database file for metagenomics identification
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/nr.dmnd.gz
diamond_db: "/idi/sabeti-scratch/yesimon/db/diamond_db/nr"

# Directory of the krona database for generating html reports.
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/krona_taxonomy_20160502.tar.gz
krona_db: "/idi/sabeti-scratch/yesimon/db/krona_taxonomy"

# BWA index prefix for metagenomic alignment.
# This contains the full human reference genome, multiple diverse sequences covering human
# pathogenic viruses, as well as LSU and SSU rRNA sequences from SILVA.
# For pre-built hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/human_viral_rrna.tar.lz4
align_rna_db: "/idi/sabeti-scratch/shared-resources/rna_bwa/human_viral_rrna"

# Directory of the NCBI taxonomy dmp files
# For pre-packaged hosted databases, you may download:
#  - https://storage.googleapis.com/sabeti-public/meta_dbs/taxonomy.tar.lz4
taxonomy_db: "/idi/sabeti-scratch/shared-resources/taxonomy"

# These are variables that must be set
env_vars:
  # The directory path to the location of the GATK jar file.
  # It must be explicitly given since GATK has to be licensed and downloaded
  # manually out of band.
  GATK_PATH: "/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.6-0-g89b7209"

  # The directory path to the Novocraft utilities, including Novoalign.
  # Without this path specified, single-threaded unlicensed Novoalign will be used.
  # To use multi-threaded Novoalign, it must be licensed and downloaded
  # manually out of band.
  NOVOALIGN_PATH: "/idi/sabeti-scratch/shared-resources/software/novocraft_v3"

# ==================== [ Values that may be changed ] ==========================

#    |----------------- Project-specific pipeline files ------------------------

# Text file containing sequencer flowcell metadata
seqruns_demux: "flowcells.txt"

# The location of a text file containing names of samples
# to be run through the depletion stages of the pipeline
samples_depletion: "samples-depletion.txt"

# The location of a text file containing the names of samples
# to be run through the assembly stages of the pipeline.
# Only samples included in this file will be run through
# assembly, and any failing samples are blocking for
# downstream steps. Removing sample names and re-running
# the pipeline will allow processing to continue past
# failures.
samples_assembly: "samples-assembly.txt"

# The location of a text file containing the names of samples
# that have failed to assemble. Not currently used for much
# beyond tracking which samples failed. Sample names removed
# from samples_assembly should be added to this file
samples_assembly_failures: "samples-assembly-failures.txt"

# The location of a text file containing names of samples to run through the
# metagenomics pipeline.
samples_metagenomics: "samples-metagenomics.txt"

# The location of a text file containing the names of sample
# names to be included in the production of VCF files
# during the ref_guided_diversity step.
samples_per_run: "samples-runs.txt"

#    |------------------- Knobs to turn to change tool execution ---------------

# The lastal tool filters input reads to be more representative of the genus of
# interest. By default, the accesstions given for the reference genome are used
# (accessions_for_ref_genome_build). Optionally, a path to a text file can be
# given, the text file itself listing accessions to be used in place of the
# reference accessions for building the lastal database. If the species of
# interest has significant genetic diversity, the lastal filtering step can be
# made more inclusive by expanding the list of accessions given in this file to
# a set that better represents possible diversity. If assemblies are failing due
# to insufficient input data, consider expanding the list of accessions given to lastal.
accessions_file_for_lastal_db_build: ""

# The minimum length an assembled sequence must have
# to be considered acceptible quality, specified as
# a fraction of the length of the reference sequene.
# This is specific to a particular segment/chromosome.
assembly_min_length_fraction_of_reference: 0.50

# The minimum fraction of unambiguous (non-N) bases an assembled
# sequence must have to be considered acceptable quality.
assembly_min_unambig: 0.50

# The replace_length is the number of bases on each end of each segment
# of the reference genome where we throw away de-novo contigs and use only
# reference-based read alignments during assembly.
#assembly_replace_length: 55

# parameters passed to Novoalign during the assembly refinement stages
refine_assembly_1_novoalign_params: "-r Random -l 30 -g 40 -x 20 -t 502"
refine_assembly_2_novoalign_params: "-r Random -l 40 -g 40 -x 20 -t 100"

# parameters passed to Novoalign for mapping reads back to the consensus
map_reads_to_self_novoalign_params: "-r Random -l 40 -g 40 -x 20 -t 100 -k"

# parameters passed to Novoalign for refining the reference-guided consensus
ref_guided_consensus_novoalign_params: "-r Random -l 30 -g 40 -x 20 -t 502"

# Alignment parameters for nucmer (MUMmer) during the scaffolding step.
# Shown below are defaults.
#assembly_nucmer_max_gap:     200
#assembly_nucmer_min_match:   10
#assembly_nucmer_min_cluster: 65
#scaffold_min_pct_contig_aligned: 0.3

# The number of threads to use for tools supporting multiple threads.
# Should equal the number of CPU cores present on the machine(s)
# running the pipeline.
number_of_threads: 8

# MAFFT offset value, which works like gap extension penalty,
# for group-to-group alignment
# See: http://mafft.cbrc.jp/alignment/software/manual/manual.html
mafft_ep: 0.123

# MAFFT number cycles of iterative refinement are performed
# See: http://mafft.cbrc.jp/alignment/software/manual/manual.html
mafft_maxiters: 1000

# The number of reads to be subsampled from input bam files
# and used as input to assembly via Trinity.
# Roughly, ~1/2 hour to 1 hour required per million reads.
# See: https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-FAQ#ques_why_so_many_transcripts
trinity_n_reads: 250000

# Minimum number of reads on each strand
vphaser_min_reads_each: 5

# Maximum allowable ratio of number of reads on the two
# strands. Ignored if vphaser_max_bins=0.
vphaser_max_bins: 10

# A simple filter for the VCF merge step.
# If set to true, keep only the alleles that have at least two
# independent libraries of support and 
# allele freq > 0.005. If false, no filtering is performed.
vcf_merge_naive_filter: false

#    |----------------------- Data storage locations ---------------------------

# The parent directory containing data sub-directories.
# Can be relative to the Snakefile location, or absolute.
data_dir: "data"

# Sub-directories within data_dir to store the input and output
# files for various stages of the pipeline.
# For raw sequencer output, the files should go in demux or source,
# depending on whether or not they have been demultiplexed
subdirs:
  demux: "00_demux"
  source: "00_raw"
  depletion: "01_cleaned"
  per_sample: "01_per_sample"
  assembly: "02_assembly"
  align_self: "02_align_to_self"
  metagenomics: "02_metagenomics"
  align_ref: "03_align_to_ref"
  multialign_ref: "03_multialign_to_ref"
  interhost: "03_interhost"
  intrahost: "04_intrahost"
  annot: "05_genbank"

# The location to use for temp file storage. Should be large and fast and
# shared between jobs.
# At the Broad, consider /broad/hptmp/myusername
tmp_dir: "tmp"

# The directory to store log files for specific jobs
log_dir: "log"

# The directory to store end-of-pipeline reports.
reports_dir: "reports"

# The directory containing the viral-ngs files (intrahost.py, taxon_filter.py, etc.)
bin_dir: "bin"

# The directory of the virtual environment to be
# activated before execution of the pipeline
conda_env_dir: "conda-env"

# The directory of a (mini)conda install
miniconda_dir: "mc3"

# The directory of where the reference genome fasta files should be stored.
ref_genome_dir: "ref_genome"

# The directory where the lastal database files should be stored
lastal_ref_db_dir: "lastal_db"

#    |------------ Values needed for automated GenBank submission --------------

# When writing output files such as sam/bam files
# The name of the sequencing center that produced the reads.
seq_center: "BI"

# Fields related to preparing files for automated GenBank submission.
genbank:
  author_template: "NCBI/authors.sbt"
  source_modifier_table: "NCBI/sample_meta.src"
  biosample_map: "NCBI/biosample-map.txt"
  sequencing_technology: "Illumina HiSeq 2500; Nextera LC"
  comment: "Please be aware that the annotation is done automatically with little or no manual curation."

# ================== [ Institution-specific parameters ] =======================

#    |----------------- Cluster execution parameters----------------------------

# The project name passed to the cluster scheduler (currently unused)
project: "viral_ngs"

# Broad-specific LSF cluster scheduler parameters
LSF_queues:
  short: "-W 4:00"
  long: "-q forest"
  bigmem: "-q flower"

# Broad-specific UGER cluster scheduler parameters
UGER_queues:
  short: "-q short"
  long: "-q long"
