"""
    These rules generate reports and metrics on reads and assemblies.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os, os.path, gzip, shutil

all_reports = [
    config["reports_dir"]+'/summary.fastqc.raw.txt',
    config["reports_dir"]+'/summary.fastqc.cleaned.txt',
    config["reports_dir"]+'/summary.fastqc.taxfilt.txt',
    config["reports_dir"]+'/summary.fastqc.align_to_self.txt'
]

if config.get("spikeins_db"):
    all_reports.append(config["reports_dir"]+'/summary.spike_count.txt')

rule all_reports:
    input: all_reports
    params: LSF="-N"

rule all_ref_guided_fastqc:
    input: config["reports_dir"]+'/summary.fastqc.align_to_ref.txt'

#-----------FASTQC---------------------

def fastqc_report_inputs(wildcards):
    if wildcards.adjective == 'raw':
        return os.path.join(config["data_dir"], config["subdirs"]["source"],
                            wildcards.sample + '.bam')
    elif wildcards.adjective in ['cleaned', 'taxfilt']:
        return os.path.join(config["data_dir"], config["subdirs"]["depletion"],
                            wildcards.sample + '.' + wildcards.adjective + '.bam')
    elif wildcards.adjective == 'align_to_self':
        return os.path.join(config["data_dir"], config["subdirs"]["align_self"],
                            wildcards.sample + '.bam')
    elif wildcards.adjective == 'align_to_ref':
        return os.path.join(config["data_dir"], config["subdirs"]["align_ref"],
                            wildcards.sample + '.realigned.only_aligned.bam')
    else:
        return "unreachable-{}-{}".format(wildcards.sample, wildcards.adjective)

rule fastqc_report:
    input:  
        fastqc_report_inputs
    output: 
        config["reports_dir"]+'/fastqc/{sample}/{adjective}'
    resources: 
        mem_mb = 3*1000
    params: 
        LSF   = config.get('LSF_queues', {}).get('short', '-W 4:00'),
        UGER  = config.get('UGER_queues', {}).get('short', '-l h_rt=04:00:00'),
        logid = "{sample}-{adjective}"
    run:
        makedirs(config["reports_dir"])
        shutil.rmtree(output[0], ignore_errors=True)
        makedirs(os.path.join(config["reports_dir"], 'fastqc'))
        shell("fastqc -f bam {input} -o {config[reports_dir]}/fastqc/{wildcards.sample}")
        fastqc_out = os.path.basename(input[0])[:-len('.bam')] + '_fastqc'
        shell("unzip {config[reports_dir]}/fastqc/{wildcards.sample}/" + fastqc_out + ".zip -d {config[reports_dir]}/fastqc/{wildcards.sample}")
        shutil.move(os.path.join(config["reports_dir"], 'fastqc', wildcards.sample, fastqc_out),
                    os.path.join(config["reports_dir"], 'fastqc', wildcards.sample, wildcards.adjective))
        os.unlink(os.path.join(config["reports_dir"], 'fastqc',
                               wildcards.sample, fastqc_out + '.zip'))
        os.unlink(os.path.join(config["reports_dir"], 'fastqc',
                               wildcards.sample, fastqc_out + '.html'))

rule consolidate_fastqc_on_all_runs:
    input:
        expand("{{dir}}/fastqc/{sample}/{{adjective}}",
            sample=read_samples_file(config["samples_per_run"]))
    output: 
        '{dir}/summary.fastqc.{adjective,raw|cleaned|taxfilt}.txt'
    params: 
        logid="all"
    shell:  
        "{config[bin_dir]}/reports.py consolidate_fastqc {input} {output}"

rule consolidate_fastqc_on_all_assemblies:
    input:
        expand("{{dir}}/fastqc/{sample}/{{adjective}}",
            sample=read_samples_file(config["samples_assembly"]))
    output: 
        '{dir}/summary.fastqc.{adjective,align_to_self|align_to_ref}.txt'
    params: 
        logid = "all"
    shell:  
        "{config[bin_dir]}/reports.py consolidate_fastqc {input} {output}"

#-----------SPIKE-INS------------------

if config.get("spikeins_db"):
    rule spikein_report:
        input:  
            input_bam   = config["data_dir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam',
            spikeins_db = objectify_remote(config["spikeins_db"])
        output: 
            config["reports_dir"]+'/spike_count/{sample}.spike_count.txt',
            config["reports_dir"]+'/spike_count/{sample}.spike_count_top_3.txt'
        resources: 
            mem_mb = 3*1000
        params: 
            LSF   = config.get('LSF_queues', {}).get('short', '-W 4:00'),
            UGER  = config.get('UGER_queues', {}).get('short', '-l h_rt=04:00:00'),
            logid = "{sample}"
        run:
            makedirs(os.path.join(config["reports_dir"], 'spike_count'))
            makedirs(os.path.join(config["tmp_dir"], config["subdirs"]["depletion"]))
            shell("{config[bin_dir]}/read_utils.py bwamem_idxstats {input.input_bam} {input.spikeins_db} --outStats {output[0]} --minScoreToFilter 60")
            shell("sort -b -r -n -k3 {output[0]} > {config[tmp_dir]}/{output[0]}.sorted")
            shell("head -n 3 {config[tmp_dir]}/{output[0]}.sorted > {output[1]}")


    rule consolidate_spike_count:
        input:  
            expand("{{dir}}/spike_count/{sample}.spike_count.txt", \
                sample=read_samples_file(config["samples_per_run"]))
        output: 
            '{dir}/summary.spike_count.txt'
        shell:  
            "{config[bin_dir]}/reports.py consolidate_spike_count {wildcards.dir}/spike_count {output}"
