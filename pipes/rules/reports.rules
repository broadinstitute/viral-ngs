"""
    These rules generate reports and metrics on reads and assemblies.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os, os.path, gzip, shutil


rule all_reports:
    input:
        config["reportsDir"]+'/summary.coverage_ref.txt.gz',
        config["reportsDir"]+'/summary.coverage_self.txt.gz',
        config["reportsDir"]+'/summary.fastqc.txt',
        config["reportsDir"]+'/summary.bamstats.txt',
        config["reportsDir"]+'/summary.spike_count.txt'
    params: LSF="-N"

#-----------BAMSTATS-------------------

def bamstats_input_file(wildcards):
    adj = wildcards.adjective
    if adj in ('raw', 'cleaned'):
        subdir = "depletion"
    else:
        if adj.startswith('self_'):
            subdir = "align_self"
        elif adj.startswith('ref_'):
            subdir = "align_ref"
        else:
            raise Exception()
        if adj.endswith('_mapped') or adj.endswith('_rmdup') or adj.endswith('_realigned'):
            adj = adj.split('_')[-1]
        elif adj.endswith('_align'):
            adj = None
        else:
            raise Exception()
    if adj==None:
        bamfile = wildcards.sample + '.bam'
    else:
        bamfile = '.'.join([wildcards.sample, adj, 'bam'])
    return os.path.join(config["dataDir"], config["subdirs"][subdir], bamfile)

rule bamstats_report:
    input:  bamstats_input_file
    output: config["reportsDir"]+'/bamstats/{sample}.{adjective}.txt'
    params: LSF='-W 0:30 -sp 40',
            logid="{sample}-{adjective}",
            tmpf_report=config["reportsDir"]+'/bamstats/{sample}.{adjective}.tmp.txt'
    run:
            makedirs(config["reportsDir"]+'/bamstats')
            shell("use BamTools; bamtools stats -insert -in {input} > {params.tmpf_report}")
            out = []
            out.append(['Sample', wildcards.sample])
            out.append(['BAM', wildcards.adjective])
            with open(params.tmpf_report, 'rt') as inf:
                for line in inf:
                    row = line.strip().split(':')
                    if len(row)==2 and row[1]:
                        k,v = [x.strip() for x in row]
                        k = k.strip("'")
                        mo = re.match(r'^(\d+)\s+\(\S+\)$', v)
                        if mo:
                            v = mo.group(1)
                        out.append([k,v])
            with open(output[0], 'wt') as outf:
                for row in out:
                    outf.write('\t'.join(row)+'\n')
            os.unlink(params.tmpf_report)

rule consolidate_bamstats:
    input:
            expand("{{dir}}/bamstats/{sample}.{adjective}.txt",
                adjective=['raw','cleaned',
                    'ref_align', 'ref_mapped', 'ref_rmdup', 'ref_realigned'],
                sample=read_samples_file(config["samples_per_run"])) + \
            expand("{{dir}}/bamstats/{sample}.{adjective}.txt",
                adjective=['self_align', 'self_mapped', 'self_rmdup', 'self_realigned'],
                sample=read_samples_file(config["samples_assembly"]))
    output: '{dir}/summary.bamstats.txt'
    params: logid="all"
    shell:  "{config[binDir]}/reports.py consolidate_bamstats {input} {output}"


#-----------FASTQC---------------------

rule fastqc_report:
    input:  config["dataDir"]+'/'+config["subdirs"]["align_self"]+'/{sample}.bam'
    output: config["reportsDir"]+'/fastqc/{sample}_fastqc'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="{sample}"
    run:
            makedirs(config["reportsDir"])
            shutil.rmtree(output[0], ignore_errors=True)
            makedirs(os.path.join(config["reportsDir"], 'fastqc'))
            shell("/idi/sabeti-scratch/kandersen/bin/fastqc/fastqc -f bam {input} -o {config[reportsDir]}/fastqc")
            os.unlink(output[0]+'.zip')

rule consolidate_fastqc:
    input:
            expand("{{dir}}/fastqc/{sample}_fastqc",
                sample=read_samples_file(config["samples_assembly"]))
    output: '{dir}/summary.fastqc.txt'
    params: logid="all"
    shell:  "{config[binDir]}/reports.py consolidate_fastqc {input} {output}"

#-----------COVERAGE-------------------

def coverage_input_files(wildcards):
    if wildcards.adjective == 'ref':
        ref   = config["ref_genome"]
        reads = os.path.join(config["dataDir"], config["subdirs"]["align_ref"], wildcards.sample + '.realigned.bam')
    elif wildcards.adjective == 'self':
        ref   = os.path.join(config["dataDir"], config["subdirs"]["assembly"], wildcards.sample + '.fasta')
        reads = os.path.join(config["dataDir"], config["subdirs"]["align_self"], wildcards.sample + '.realigned.bam')
    else:
        raise Exception()
    return (ref, reads)

rule coverage_report:
    input:  coverage_input_files
    output: config["reportsDir"]+'/coverage/{sample}.coverage_{adjective}.txt'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00') + " -sp 40",
            logid="{sample}-{adjective}"
    run:
            makedirs(os.path.join(config["reportsDir"], 'coverage'))
            shell("/idi/sabeti-scratch/kandersen/bin/bedtools/bedtools genomecov -d -ibam {input[1]} -g {input[0]} > {output}")

def consolidate_coverage_inputs(wildcards):
    if wildcards.adjective == 'ref':
        samples = config['samples_per_run']
    elif wildcards.adjective == 'self':
        samples = config['samples_assembly']
    else:
        raise Exception()
    return [wildcards.dir+'/coverage/'+sample+'.coverage_'+wildcards.adjective+'.txt'
        for sample in read_samples_file(samples)]

rule consolidate_coverage:
    input:  consolidate_coverage_inputs
    output: '{dir}/summary.coverage_{adjective}.txt.gz'
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="all"
    shell:  "{config[binDir]}/reports.py consolidate_coverage {input} {wildcards.adjective} {output}"


#-----------SPIKE-INS------------------

rule spikein_report:
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    output: config["reportsDir"]+'/spike_count/{sample}.spike_count.txt'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="{sample}",
            spike_in_fasta=config["spikeinsDb"],
            novoalign_options="-r Random",
            tmpf_spike_bam=config["tmpDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.aligned_to_spikes.bam'
    run:
            makedirs(os.path.join(config["reportsDir"], 'spike_count'))
            makedirs(os.path.join(config["tmpDir"], config["subdirs"]["depletion"]))
            shell("{config[binDir]}/read_utils.py novoalign {input} {params.spike_in_fasta} {params.tmpf_spike_bam} --options '{params.novoalign_options}'")
            shell("/idi/sabeti-scratch/kandersen/bin/scripts/CountAlignmentsByDescriptionLine -bam {params.tmpf_spike_bam} > {output}")
            os.unlink(params.tmpf_spike_bam)
            os.unlink(params.tmpf_spike_bam[:-1] + 'i')

rule consolidate_spike_count:
    input:  expand("{{dir}}/spike_count/{sample}.spike_count.txt", \
                sample=read_samples_file(config["samples_per_run"]))
    output: '{dir}/summary.spike_count.txt'
    params: logid="all"
    shell:  "{config[binDir]}/reports.py consolidate_spike_count {input} {output}"

