"""
    This is a basic framework for depleting human and other contaminant 
    reads from NGS data.  All non-human reads should remain behind.
	
	note: the length of runtime per step is highly variable depending on
	the size of the input data. Eventually replace with something TANGO-based.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os

rule all_deplete:
    input:
        expand("{dataDir}/{subdir}/{sample}.{adjective}.bam",
            dataDir=config["dataDir"],
            subdir=config["subdirs"]["per_sample"],
            adjective=['cleaned','taxfilt'],
            sample=read_samples_file(config["samples_depletion"])),
    params: LSF="-N"


"""
rule revert_bam:
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["tmpDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/read_utils.py revert_bam_picard {input} {output} --picardOptions SORT_ORDER=queryname SANITIZE=true")

rule deplete_bmtagger:
    ''' This step depletes human reads using BMTagger
        This sometimes takes just over 4 hrs for highly dense lanes
    '''
    input:  config["tmpDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam'
    #       expand("{dbdir}/{db}.{suffix}",
    #           dbdir=config["bmTaggerDbDir"],
    #           db=config["bmTaggerDbs_remove"],
    #           suffix=["bitmask","srprism.idx","srprism.map"])
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    resources: mem=10
    params: LSF=config.get('LSF_queues', {}).get('long', '-q forest'),
            refDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_bam_bmtagger {input} {params.refDbs} {output}"

rule rmdup_mvicuna:
    ''' This step removes PCR duplicate reads using M-Vicuna (our custom
        tool included here).  
        Unlike Picard MarkDuplicates, M-Vicuna does not require reads to be
        previously aligned.
    '''
    input:  config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam'
    output: config["tmpDir"]+ '/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="{sample}"
    shell:  "{config[binDir]}/read_utils.py rmdup_mvicuna_bam {input} {output}"

rule deplete_blastn:
    ''' This step depletes human reads using BLASTN, which is more sensitive,
        but much slower than BMTagger, so we run it last.
        This sometimes takes just over 4 hrs for highly dense lanes.
    '''
    input:  config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=15
    params: LSF=config.get('LSF_queues', {}).get('long', '-q forest'),
            refDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py deplete_blastn_bam {input} {params.refDbs} {output}"
"""


rule depletion:
    ''' Runs a full human read depletion pipeline and removes PCR duplicates
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam'
    output: config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam',
            config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.rmdup.bam',
            config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: mem=15
    params: LSF=config.get('LSF_queues', {}).get('long', '-q forest'),
            bmtaggerDbs = expand("{dbdir}/{db}", dbdir=config["bmTaggerDbDir"], db=config["bmTaggerDbs_remove"]),
            blastDbs = expand("{dbdir}/{db}", dbdir=config["blastDbDir"], db=config["blastDb_remove"]),
            revert_bam = config["tmpDir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam',
            logid="{sample}"
    run:
            makedirs(expand("{dir}/{subdir}",
                dir=[config["dataDir"],config["tmpDir"]],
                subdir=config["subdirs"]["depletion"]))
            shell("{config[binDir]}/taxon_filter.py deplete_human {input} {params.revert_bam} {output} --bmtaggerDbs {params.bmtaggerDbs} --blastDbs {params.blastDbs}")
            os.unlink(params.revert_bam)

rule filter_to_taxon:
    ''' This step reduces the read set to a specific taxon (usually the genus
        level or greater for the virus of interest).
    '''
    input:  config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    output: config["dataDir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.taxfilt.bam'
    resources: mem=3
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            refDb=config["lastal_refDb"],
            logid="{sample}"
    shell:  "{config[binDir]}/taxon_filter.py filter_lastal_bam {input} {params.refDb} {output}"

def merge_one_per_sample_inputs(wildcards):
    if 'seqruns_demux' not in config or not os.path.isfile(config['seqruns_demux']):
        return config["dataDir"]+'/'+config["subdirs"]["depletion"] +'/'+ wildcards.sample + '.' + wildcards.adjective + '.bam'
    runs = set()
    for flowcell in read_samples_file(config['seqruns_demux']):
        for lane in read_tab_file(flowcell):
            for well in read_tab_file(lane['barcode_file']):
                if well['sample'] == wildcards.sample:
                  if wildcards.adjective=='raw':
                     runs.add(os.path.join(config["dataDir"], config["subdirs"]["source"],
                        get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] + '.bam'))
                  else:
                     runs.add(os.path.join(config["dataDir"], config["subdirs"]["depletion"],
                        get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] +'.'+ wildcards.adjective + '.bam'))
    return sorted(runs)
rule merge_one_per_sample:
    ''' All of the above depletion steps are run once per flowcell per lane per
        multiplexed sample.  This reduces recomputation on the same data when
        additional sequencing runs are performed on the same samples.  This
        step merges reads down to one-per-sample, which all downstream
        analysis steps require.  For cleaned and taxfilt outputs, we re-run
        the rmdup step on a per-library basis after merging.
    '''
    input:  merge_one_per_sample_inputs
    output: config["dataDir"]+'/'+config["subdirs"]["per_sample"] +'/{sample}.{adjective,raw|cleaned|taxfilt}.bam'
    resources: mem=7
    params: LSF=config.get('LSF_queues', {}).get('short', '-W 4:00'),
            logid="{sample}-{adjective}",
            tmpf_bam=config["tmpDir"]+'/'+config["subdirs"]["depletion"] +'/{sample}.{adjective}.bam'
    run:
            makedirs(config["dataDir"]+'/'+config["subdirs"]["per_sample"])
            if wildcards.adjective == 'raw':
                shell("{config[binDir]}/read_utils.py merge_bams {input} {output} --picardOptions SORT_ORDER=queryname")
            else:
                shell("{config[binDir]}/read_utils.py merge_bams {input} {params.tmpf_bam} --picardOptions SORT_ORDER=queryname")
                shell("{config[binDir]}/read_utils.py rmdup_mvicuna_bam {params.tmpf_bam} {output} --JVMmemory 8g")

