"""
    This is a basic framework for depleting human and other contaminant 
    reads from NGS data.  All non-human reads should remain behind.
    
    note: runtime and memory per step can be highly variable depending on
    the size of the input data.
"""

__author__ = 'Kristian Andersen <andersen@broadinstitute.org>, Daniel Park <dpark@broadinstitute.org>'

from snakemake.utils import makedirs
import os

rule all_deplete:
    input:
        expand("{data_dir}/{subdir}/{sample}.{adjective}.bam",
            data_dir=config["data_dir"],
            subdir=config["subdirs"]["per_sample"],
            adjective=['cleaned','taxfilt'],
            sample=read_samples_file(config["samples_depletion"])),
    params: LSF="-N"

rule depletion:
    ''' Runs a full human read depletion pipeline and removes PCR duplicates
    '''
    input:  
        input_bam    = config["data_dir"]+'/'+config["subdirs"]["source"]+'/{sample}.bam',
        bmtagger_dbs = objectify_remote(expand("{db}.{ext}", db=config["bmtagger_dbs_remove"], ext=['bitmask','srprism.ssa','srprism.imp','srprism.idx','srprism.map','srprism.ss','srprism.amp','srprism.rmp','srprism.pmp','srprism.ssd'])),
        blast_dbs    = objectify_remote(expand("{db}.{ext}", db=config["blast_db_remove"], ext=['nhr','nsq','nin'])), # ['nhr','nix','nsq','dict','fai','nin']
        bwa_dbs      = objectify_remote(expand("{db}.{ext}", db=config["bwa_dbs_remove"], ext=['bwt','amb','ann','pac','sa']))
    output: 
        config["tmp_dir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.bwa_depleted.bam',
        config["tmp_dir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.bmtagger_depleted.bam',
        config["data_dir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam'
    resources: 
        mem_mb     = 15*1000,
        threads = int(config.get("number_of_threads", 1))
    params: 
        LSF        = config.get('LSF_queues', {}).get('long', '-q forest'),
        UGER       = config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00'),
        revert_bam = config["tmp_dir"] +'/'+config["subdirs"]["depletion"]+'/{sample}.raw.bam',
        logid      = "{sample}"
    run:
        makedirs(expand("{dir}/{subdir}",
            dir=[config["data_dir"],config["tmp_dir"]],
            subdir=config["subdirs"]["depletion"]))
        mem_mb = int(resources.mem_mb * 0.95)
        blast_db_prefixes=" ".join(set([strip_protocol(dbf, relative=True) for dbf in config["blast_db_remove"]]))
        bmtagger_db_prefixes=" ".join(set([strip_protocol(dbf, relative=True) for dbf in config["bmtagger_dbs_remove"]]))
        bwa_db_prefixes=" ".join(set([strip_protocol(dbf, relative=True) for dbf in config["bwa_dbs_remove"]]))
        shell("{config[bin_dir]}/taxon_filter.py deplete {input.input_bam} {params.revert_bam} {output} --bwaDbs {bwa_db_prefixes} --bmtaggerDbs {bmtagger_db_prefixes} --blastDbs {blast_db_prefixes} --threads {resources.threads} --srprismMemory {mem_mb} --JVMmemory {resources.mem_mb}m")
        os.unlink(params.revert_bam)

rule filter_to_taxon:
    ''' This step reduces the read set to a specific taxon (usually the genus
        level or greater for the virus of interest).
    '''
    input:  
        input_bam = config["data_dir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.cleaned.bam',
        lastalDb  = objectify_remote(expand('{lastal_ref_db_dir}/lastal.fasta', lastal_ref_db_dir=config["lastal_ref_db_dir"])),
    output: 
        config["data_dir"]+'/'+config["subdirs"]["depletion"]+'/{sample}.taxfilt.bam'
    resources: 
        mem_mb = 7*1000
    params: 
        LSF   = config.get('LSF_queues', {}).get('short', '-W 4:00'),
        UGER  = config.get('UGER_queues', {}).get('short', '-l h_rt=2:00:00'),
        logid = "{sample}"
    run: 
        shell("{config[bin_dir]}/taxon_filter.py filter_lastal_bam {input.input_bam} {input.lastalDb} {output}")


class MergeInputException(Exception):
    '''Cannot find merging multiplexed sample inputs'''


def merge_one_per_sample_inputs(wildcards):
    if 'seqruns_demux' not in config or not os.path.isfile(config['seqruns_demux']):
        if wildcards.adjective == 'raw':
            return config["data_dir"] + '/' + config["subdirs"]["source"] + '/' + wildcards.sample + '.bam'
        else:
            return config["data_dir"]+'/'+config["subdirs"]["depletion"] +'/'+ wildcards.sample + '.' + wildcards.adjective + '.bam'
    runs = set()
    for lane in read_tab_file(config['seqruns_demux']):
        for well in read_tab_file(lane['barcode_file']):
            if well['sample'] == wildcards.sample:
              if wildcards.adjective=='raw':
                 runs.add(os.path.join(config["data_dir"], config["subdirs"]["source"],
                    get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] + '.bam'))
              else:
                 runs.add(os.path.join(config["data_dir"], config["subdirs"]["depletion"],
                    get_run_id(well) +'.'+ lane['flowcell'] +'.'+ lane['lane'] +'.'+ wildcards.adjective + '.bam'))
    if not runs:
        raise MergeInputException(
            "Cannot find inputs to merge into '{}.{}.bam'. Make sure to check 'flowcells.txt' and 'barcodes.txt' "
            "to ensure they match up with the sample bams.".format(wildcards.sample, wildcards.adjective))
    return sorted(runs)


def merge_one_per_sample_queue(_, input):
    total_size = 0
    for fn in input:
        if not os.path.exists(fn):
            return config.get('UGER_queues', {}).get('short', '-l h_rt=04:00:00')
        total_size += os.stat(fn).st_size
    if total_size > 1500000000:
        return config.get('UGER_queues', {}).get('long', '-l h_rt=36:00:00')
    return config.get('UGER_queues', {}).get('short', '-l h_rt=04:00:00')


rule merge_one_per_sample:
    ''' All of the above depletion steps are run once per flowcell per lane per
        multiplexed sample.  This reduces recomputation on the same data when
        additional sequencing runs are performed on the same samples.  This
        step merges reads down to one-per-sample, which all downstream
        analysis steps require.  For cleaned and taxfilt outputs, we re-run
        the rmdup step on a per-library basis after merging.
    '''
    input:  
        merge_one_per_sample_inputs
    output: 
        config["data_dir"]+'/'+config["subdirs"]["per_sample"] +'/{sample}.{adjective,raw|cleaned|taxfilt}.bam'
    resources: 
        mem_mb = 10*1000
    params: 
        LSF      = config.get('LSF_queues', {}).get('short', '-W 4:00'),
        UGER     = merge_one_per_sample_queue,
        logid    = "{sample}-{adjective}",
        tmpf_bam = config["tmp_dir"]+'/'+config["subdirs"]["depletion"] +'/{sample}.{adjective}.bam'
    run:
        mem_mb = int(resources.mem_mb * 0.90)
        makedirs(config["data_dir"]+'/'+config["subdirs"]["per_sample"])
        if wildcards.adjective == 'raw':
            shell("{config[bin_dir]}/read_utils.py merge_bams {input} {output} --picardOptions SORT_ORDER=queryname")
        else:
            shell("{config[bin_dir]}/read_utils.py merge_bams {input} {params.tmpf_bam} --picardOptions SORT_ORDER=queryname")
            shell("{config[bin_dir]}/read_utils.py rmdup_clumpify_bam {params.tmpf_bam} {output} --JVMmemory {mem_mb}m")
