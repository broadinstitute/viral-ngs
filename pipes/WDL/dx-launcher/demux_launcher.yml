# DNAnexus applet to build with dx-yml-build which can be found here:
# https://gist.githubusercontent.com/mlin/3cce81f54a640c3f62a2725acbc98283/raw/e071dfe1989c31f6267334106115620770e6d21c/dx-yml-build
# To build, save this file as dxapp.yml and run that script alongside.
name: demux_launcher
dxapi: 1.0.0
version: 0.0.1
description: Launches demux/demux-plus workflow on each lane of a sequencing run, given the incremental uploader sentinel record.
inputSpec:
- name: upload_sentinel_record
  class: record
  type: "UploadSentinel"
  help: Sentinel record from incremental upload tool. The RunInfo.xml and run tarballs must also reside in the current project.
- name: demux_workflow_id
  class: string
  help: DNAnexus ID (workflow-xxxx) of the demux/demux-plus workflow to launch on each lane of the run. The workflow and all its parts and dependencies must reside in the current project.
  default: DEFAULT_DEMUX_WORKFLOW_ID
- name: folder
  class: string
  default: /
  help: Output folder in the root project. For this applet, the --destination should be set to / and this input should be used to control the output folder.
- name: api_token
  class: file
  optional: true
  help: Optional- to launch each lane demux as a top-level analysis instead of a sub-job, so that the analyses succeed or fail independently, provide a text file containing a DNAnexus API token with at least CONTRIBUTE access to the current project. Protect this file by isolating it in a separate project, viewable only by users who will invoke this launcher.
outputSpec:
- name: run_tarball
  class: file
  help: If the run was uploaded in multiple partial tarballs, they're consolidated and output here. Otherwise the uploaded tarball is echoed.
- name: demux_outputs
  class: array:file
  help: All files output from the demux/demux-plus analyses
access:
  network: ["*"]
runSpec:
  systemRequirements:
    main:
      instanceType: mem1_ssd1_x2
    launch_demux:
      instanceType: mem1_ssd1_x2
  distribution: Ubuntu
  release: "16.04"
  execDepends:
    - name: pigz
    - name: libxml2-utils
  interpreter: bash
  code: |
    #!/bin/bash

    main() {
      set -ex -o pipefail

      # Download the RunInfo.xml file
      runInfo_file_id=$(dx get_details "$upload_sentinel_record" | jq .runinfo_file_id -r)
      dx cat $runInfo_file_id > RunInfo.xml

      if [ -n "$api_token" ]; then
        dx download -o api_token "$api_token"
      fi

      # Parse the lane count & run ID from RunInfo.xml file
      lane_count=$(xmllint --xpath "string(//Run/FlowcellLayout/@LaneCount)" RunInfo.xml)
      if [ -z "$lane_count" ]; then
          dx-jobutil-report-error "Could not parse LaneCount from RunInfo.xml. Please check RunInfo.xml is properly formatted"
      fi
      run_id=$(xmllint --xpath "string(//Run/@Id)" RunInfo.xml)
      if [ -z "$run_id" ]; then
          dx-jobutil-report-error "Could not parse Run@Id from RunInfo.xml. Please check RunInfo.xml is properly formatted"
      fi

      run_tarball=""
      dx get_details "$upload_sentinel_record" | jq -r .tar_file_ids[] > tar_file_ids
      if [ "$(cat tar_file_ids | wc -l)" -gt 1 ]; then
        # if there are multiple run tarballs, launch a subjob to consolidate them
        # TODO: auto-scale the instance type, here hardcoding one with 640GB space
        subjob=$(dx-jobutil-new-job consolidate_tars --instance-type mem1_ssd2_x8 -i upload_sentinel_record="$upload_sentinel_record" -i folder="$folder" -i run_id="$run_id")
        run_tarball="$subjob:run_tarball"
      else
        run_tarball=$(cat tar_file_ids | tr -d '\n')
      fi
      dx-jobutil-add-output run_tarball $run_tarball

      if [ -n "$demux_workflow_id" ]; then
        # launch the demux workflow on each lane
        analyses=()
        output_project=$DX_WORKSPACE_ID
        for i in $(seq "$lane_count"); do
          folder2=$(printf "%s/%s/L%d" "$folder" "$run_id" $i)
          runcmd="dx run $demux_workflow_id -i illumina_demux.flowcell_tgz=$run_tarball -i illumina_demux.lane=$i --folder $folder2 --name demux:$run_id:L$i -y --brief"
          echo "$runcmd"
          set +x
          if [ -n "$api_token" ]; then
            # add API token to run command without copying it into the job log
            runcmd="unset DX_JOB_ID; $runcmd --auth-token $(cat api_token | tr -d '\n')"
            output_project=$DX_PROJECT_CONTEXT_ID
          fi
          analysis=$(bash -e -o pipefail -c "$runcmd")
          set -x
          analyses+=($analysis)
        done

        # schedule a subjob when all the demux analyses complete, to propagate all their
        # output files to the output of this job.
        subjob=$(dx-jobutil-new-job propagate_outputs -i output_project=$output_project -i folder="$folder" -i run_id="$run_id" --depends-on ${analyses[@]})
        dx-jobutil-add-output demux_outputs $subjob:demux_outputs
      fi
    }

    consolidate_tars() {
      set -ex -o pipefail

      # sequentially unpack the run tarballs
      dx get_details "$upload_sentinel_record" | jq -r .tar_file_ids[] > tar_file_ids
      mkdir run/
      while read tar_file_id; do
        dx cat "$tar_file_id" | pigz -dc | tar xf - -C run/ --owner root --group root --no-same-owner
      done < tar_file_ids
      du -sh run/

      # tar the consolidated directory and upload
      # TODO: consider zstd/lz4
      dx mkdir -p "$folder/$run_id"
      tar_id=$(tar c -C run/ . | pigz -c | dx upload --brief -p --destination $(printf "%s/%s/%s.tar.gz" "$folder" "$run_id" "$run_id") -)
      dx-jobutil-add-output run_tarball "$tar_id"
    }

    propagate_outputs() {
      set -ex -o pipefail
      dx find data --brief --project $output_project --folder "$folder/$run_id" | cut -d ':' -f2 |  xargs -t -i dx-jobutil-add-output demux_outputs --class array:file "{}"
    }
